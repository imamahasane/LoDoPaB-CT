{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7ce123-fb87-4cb4-a1ba-75fcc1cd217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac33160-cdb8-46f7-a7de-9d8e6f440c86",
   "metadata": {},
   "source": [
    "observation_test/\n",
    "ground_truth_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3b3f89-d4a3-4730-93aa-98d483510857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Paths to your dataset folders (adjust to your actual directory)\n",
    "BASE_PATH = \"\"  # Replace with your actual path\n",
    "# TRAIN_OBS_DIR = os.path.join(BASE_PATH, \"observation_train\")\n",
    "# TRAIN_GT_DIR = os.path.join(BASE_PATH, \"ground_truth_train\")\n",
    "\n",
    "TRAIN_OBS_DIR = os.path.join(BASE_PATH, \"observation_test\")\n",
    "TRAIN_GT_DIR = os.path.join(BASE_PATH, \"ground_truth_test\")\n",
    "\n",
    "# Custom Dataset class for LoDoPaB-CT\n",
    "class LoDoPaBDataset(Dataset):\n",
    "    def __init__(self, obs_dir, gt_dir, num_files=None):\n",
    "        self.obs_files = sorted(os.listdir(obs_dir))\n",
    "        if num_files:\n",
    "            self.obs_files = self.obs_files[:num_files]\n",
    "        self.obs_dir = obs_dir\n",
    "        self.gt_dir = gt_dir\n",
    "\n",
    "        print(\"Loading dataset...\")\n",
    "        self.obs_data = []\n",
    "        self.gt_data = []\n",
    "        for file_name in self.obs_files:\n",
    "            obs_file = os.path.join(obs_dir, file_name)\n",
    "            gt_file = os.path.join(gt_dir, file_name.replace(\"observation\", \"ground_truth\"))\n",
    "            with h5py.File(obs_file, 'r') as f_obs, h5py.File(gt_file, 'r') as f_gt:\n",
    "                obs = f_obs['data'][:].astype(np.float16)\n",
    "                gt = f_gt['data'][:].astype(np.float16)\n",
    "                self.obs_data.append(obs)\n",
    "                self.gt_data.append(gt)\n",
    "        \n",
    "        self.obs_data = np.concatenate(self.obs_data, axis=0)\n",
    "        self.gt_data = np.concatenate(self.gt_data, axis=0)\n",
    "        \n",
    "        self.obs_data = (self.obs_data - np.min(self.obs_data)) / (np.max(self.obs_data) - np.min(self.obs_data)) * 2 - 1\n",
    "        self.gt_data = (self.gt_data - np.min(self.gt_data)) / (np.max(self.gt_data) - np.min(self.gt_data)) * 2 - 1\n",
    "        print(f\"Dataset loaded: {self.obs_data.shape[0]} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obs = torch.FloatTensor(self.obs_data[idx]).unsqueeze(0)  # [1, 1000, 513]\n",
    "        gt = torch.FloatTensor(self.gt_data[idx]).unsqueeze(0)    # [1, 362, 362]\n",
    "        return obs, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3ad640",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acea0927-acbd-4056-a60b-52a85c33ed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 3553 samples\n",
      "Dataset size: 3553 samples\n",
      "Number of batches: 297\n"
     ]
    }
   ],
   "source": [
    "dataset = LoDoPaBDataset(TRAIN_OBS_DIR, TRAIN_GT_DIR)\n",
    "dataloader = DataLoader(dataset, batch_size=12, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Test dataloader\n",
    "# print(\"Testing dataloader...\")\n",
    "# for i, (obs, gt) in enumerate(dataloader):\n",
    "#     print(f\"Batch {i+1}/{len(dataloader)} - Obs shape: {obs.shape}, GT shape: {gt.shape}\")\n",
    "#     break\n",
    "# print(\"Dataloader test complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82548465-35a5-4676-8b8a-34aa5a0d7124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mahfuz_17\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mahfuz_17\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg16\n",
    "from piqa import SSIM, LPIPS\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom modules must be implemented separately\n",
    "class MultiKernelDepthwiseConv(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ConvolutionalMultiFocalAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.conv(x)) * x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, img_size=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # First two convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Global average pooling (to reduce the feature map size)\n",
    "        self.global_pooling = nn.AdaptiveAvgPool2d(1)  # Outputs a 1x1 feature map per channel\n",
    "        \n",
    "        # Fully connected layer after the global pooling\n",
    "        self.fc = nn.Linear(128, 1)  # The output size is the number of channels after pooling\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        \n",
    "        # Apply global average pooling\n",
    "        x = self.global_pooling(x)\n",
    "        \n",
    "        # Flatten the output of the global pooling (this will be 2D now)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, channels)\n",
    "        \n",
    "        x = self.fc(x)  # Apply the fully connected layer\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UltraLightUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, channels=[16, 32, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channels[0], kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.encoder1 = self._make_encoder_block(channels[0], channels[0])\n",
    "        self.encoder2 = self._make_encoder_block(channels[0], channels[1], downsample=True)\n",
    "        self.encoder3 = self._make_encoder_block(channels[1], channels[2], downsample=True)\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(channels[2], channels[1], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels[1]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(channels[1], channels[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(channels[0], out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def _make_encoder_block(self, in_ch, out_ch, downsample=False):\n",
    "        layers = []\n",
    "        if downsample:\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        layers.extend([\n",
    "            nn.Conv2d(in_ch, in_ch, kernel_size=3, padding=1, groups=in_ch),  # Depthwise Conv\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_size = (x.size(2), x.size(3))  # Save input size\n",
    "        x = self.initial_conv(x)  # [8, 16, 256, 256]\n",
    "        e1 = self.encoder1(x)     # [8, 16, 256, 256]\n",
    "        e2 = self.encoder2(e1)    # [8, 32, 128, 128]\n",
    "        e3 = self.encoder3(e2)    # [8, 64, 64, 64]\n",
    "        \n",
    "        d2 = self.up2(e3)         # [8, 32, 128, 128]\n",
    "        e2_resized = F.interpolate(e2, size=(d2.size(2), d2.size(3)), mode='bilinear', align_corners=True)\n",
    "        d2 = d2 + e2_resized      # [8, 32, 128, 128]\n",
    "        \n",
    "        d1 = self.up1(d2)         # [8, 16, 256, 256]\n",
    "        e1_resized = F.interpolate(e1, size=(d1.size(2), d1.size(3)), mode='bilinear', align_corners=True)\n",
    "        d1 = d1 + e1_resized      # [8, 16, 256, 256]\n",
    "        \n",
    "        output = self.final_conv(d1)  # [8, 1, 256, 256]\n",
    "        output = F.interpolate(output, size=original_size, mode='bilinear', align_corners=True)  # [8, 1, 513, 513]\n",
    "        return output\n",
    "\n",
    "# GAN Losses\n",
    "class GANLosses:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.adv_loss = nn.BCELoss()\n",
    "        self.vgg = vgg16(pretrained=True).features[:16].to(device).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.pixel_loss = nn.L1Loss()\n",
    "        self.ssim = SSIM().to(device)\n",
    "        self.lpips = LPIPS().to(device)\n",
    "        \n",
    "    def adversarial_loss(self, pred, is_real):\n",
    "        target = torch.ones_like(pred) if is_real else torch.zeros_like(pred)\n",
    "        return self.adv_loss(pred, target)\n",
    "    \n",
    "    def perceptual_loss(self, generated, target):\n",
    "        gen_rgb = generated.repeat(1, 3, 1, 1)\n",
    "        target_rgb = target.repeat(1, 3, 1, 1)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)\n",
    "        gen_rgb = (gen_rgb - mean) / std\n",
    "        target_rgb = (target_rgb - mean) / std\n",
    "        gen_features = self.vgg(gen_rgb)\n",
    "        target_features = self.vgg(target_rgb)\n",
    "        return F.mse_loss(gen_features, target_features)\n",
    "    \n",
    "    def compute_metrics(self, generated, target):\n",
    "        with torch.no_grad():\n",
    "            gen = (generated + 1) / 2\n",
    "            tgt = (target + 1) / 2\n",
    "            psnr = 10 * torch.log10(1 / F.mse_loss(gen, tgt))\n",
    "            ssim_val = self.ssim(gen, tgt)\n",
    "            lpips_val = self.lpips(gen, tgt)\n",
    "            return {'PSNR': psnr.item(), 'SSIM': ssim_val.item(), 'LPIPS': lpips_val.item()}\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def train_gan(generator, discriminator, loss_fn, optimizer_G, optimizer_D, dataloader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()  # Start timer for epoch\n",
    "\n",
    "        for batch in dataloader:\n",
    "            real_images = batch[0].to(device)  # Ensure tensor, not list\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_images = generator(torch.randn_like(real_images))\n",
    "\n",
    "            # --- Discriminator Update ---\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = loss_fn.adversarial_loss(discriminator(real_images), True)\n",
    "            fake_loss = loss_fn.adversarial_loss(discriminator(fake_images.detach()), False)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --- Generator Update ---\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = loss_fn.adversarial_loss(discriminator(fake_images), True) + loss_fn.pixel_loss(fake_images, real_images)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        epoch_time = time.time() - start_time  # Calculate epoch duration\n",
    "\n",
    "        # Print epoch stats with time\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "            f\"D Loss: {d_loss.item():.4f}, \"\n",
    "            f\"G Loss: {g_loss.item():.4f}, \"\n",
    "            f\"Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UltraLightUNetGenerator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "loss_fn = GANLosses(device)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d304b04-4c90-4796-92be-b521d9ff682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], D Loss: 0.5266, G Loss: 1.1816, Time: 102.81s\n",
      "Epoch [2/50], D Loss: 0.5097, G Loss: 1.1337, Time: 89.20s\n",
      "Epoch [3/50], D Loss: 0.4979, G Loss: 1.2479, Time: 95.65s\n",
      "Epoch [4/50], D Loss: 0.3784, G Loss: 1.4698, Time: 105.16s\n",
      "Epoch [5/50], D Loss: 0.3341, G Loss: 1.4819, Time: 107.37s\n",
      "Epoch [6/50], D Loss: 0.2957, G Loss: 1.6395, Time: 108.44s\n",
      "Epoch [7/50], D Loss: 0.2432, G Loss: 1.7219, Time: 107.13s\n",
      "Epoch [8/50], D Loss: 0.1843, G Loss: 1.9917, Time: 107.94s\n",
      "Epoch [9/50], D Loss: 0.1375, G Loss: 2.2515, Time: 107.59s\n",
      "Epoch [10/50], D Loss: 0.1557, G Loss: 2.0495, Time: 107.32s\n",
      "Epoch [11/50], D Loss: 0.1434, G Loss: 2.1068, Time: 107.36s\n",
      "Epoch [12/50], D Loss: 0.1930, G Loss: 2.1741, Time: 107.49s\n",
      "Epoch [13/50], D Loss: 0.1509, G Loss: 2.5935, Time: 106.27s\n",
      "Epoch [14/50], D Loss: 0.1057, G Loss: 2.6741, Time: 106.38s\n",
      "Epoch [15/50], D Loss: 0.1065, G Loss: 2.5543, Time: 106.32s\n",
      "Epoch [16/50], D Loss: 0.0528, G Loss: 3.4050, Time: 107.67s\n",
      "Epoch [17/50], D Loss: 0.1122, G Loss: 2.6794, Time: 107.52s\n",
      "Epoch [18/50], D Loss: 0.0586, G Loss: 3.4860, Time: 97.33s\n",
      "Epoch [19/50], D Loss: 0.0691, G Loss: 2.9461, Time: 93.81s\n",
      "Epoch [20/50], D Loss: 0.0921, G Loss: 2.9323, Time: 93.71s\n",
      "Epoch [21/50], D Loss: 0.1248, G Loss: 2.6361, Time: 94.13s\n",
      "Epoch [22/50], D Loss: 0.0497, G Loss: 3.2387, Time: 94.38s\n",
      "Epoch [23/50], D Loss: 0.0532, G Loss: 3.3467, Time: 93.77s\n",
      "Epoch [24/50], D Loss: 0.0695, G Loss: 2.6848, Time: 93.64s\n",
      "Epoch [25/50], D Loss: 0.0256, G Loss: 3.8955, Time: 93.71s\n",
      "Epoch [26/50], D Loss: 0.0608, G Loss: 3.3127, Time: 93.61s\n",
      "Epoch [27/50], D Loss: 0.0370, G Loss: 3.6775, Time: 93.63s\n",
      "Epoch [28/50], D Loss: 0.0295, G Loss: 3.8133, Time: 93.69s\n",
      "Epoch [29/50], D Loss: 0.1267, G Loss: 2.4501, Time: 93.61s\n",
      "Epoch [30/50], D Loss: 0.0377, G Loss: 3.6650, Time: 93.68s\n",
      "Epoch [31/50], D Loss: 0.0284, G Loss: 3.6859, Time: 93.53s\n",
      "Epoch [32/50], D Loss: 0.0408, G Loss: 3.3905, Time: 93.64s\n",
      "Epoch [33/50], D Loss: 0.0280, G Loss: 4.1401, Time: 93.58s\n",
      "Epoch [34/50], D Loss: 0.0136, G Loss: 4.4025, Time: 93.65s\n",
      "Epoch [35/50], D Loss: 0.0556, G Loss: 3.1819, Time: 93.83s\n",
      "Epoch [36/50], D Loss: 0.0101, G Loss: 4.6574, Time: 93.86s\n",
      "Epoch [37/50], D Loss: 0.0447, G Loss: 2.9060, Time: 93.90s\n",
      "Epoch [38/50], D Loss: 0.0328, G Loss: 3.8946, Time: 93.90s\n",
      "Epoch [39/50], D Loss: 0.1358, G Loss: 2.5606, Time: 93.87s\n",
      "Epoch [40/50], D Loss: 0.0835, G Loss: 2.9899, Time: 93.89s\n",
      "Epoch [41/50], D Loss: 0.0216, G Loss: 4.2988, Time: 93.87s\n",
      "Epoch [42/50], D Loss: 0.0240, G Loss: 3.8900, Time: 93.86s\n",
      "Epoch [43/50], D Loss: 0.0119, G Loss: 4.6907, Time: 93.98s\n",
      "Epoch [44/50], D Loss: 0.0293, G Loss: 3.7643, Time: 93.14s\n",
      "Epoch [45/50], D Loss: 0.0380, G Loss: 4.0040, Time: 90.47s\n",
      "Epoch [46/50], D Loss: 0.0393, G Loss: 3.6848, Time: 90.48s\n",
      "Epoch [47/50], D Loss: 0.0194, G Loss: 4.3768, Time: 90.43s\n",
      "Epoch [48/50], D Loss: 0.0146, G Loss: 4.4333, Time: 90.47s\n",
      "Epoch [49/50], D Loss: 0.0381, G Loss: 3.4036, Time: 90.40s\n",
      "Epoch [50/50], D Loss: 0.0111, G Loss: 5.0491, Time: 90.48s\n"
     ]
    }
   ],
   "source": [
    "train_gan(generator, discriminator, loss_fn, optimizer_G, optimizer_D, dataloader, 50, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe38263d-9535-4ae0-a7b7-7fe63ab582a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 1, 513, 513])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UltraLightUNetGenerator().to(device)\n",
    "sample_input = torch.randn(8, 1, 513, 513).to(device)\n",
    "output = generator(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be [8, 1, 513, 513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "12502753-d085-4700-b8c0-cbe7ac632bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UltraLightUNetGenerator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f6b90a7-1037-4838-99e9-883814ee325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.randn(8, 1, 512, 512).to(device)\n",
    "output = generator(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "091c9d56-0e18-411e-ae8b-309841fffd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 500, 254]             800\n",
      "       BatchNorm2d-2         [-1, 16, 500, 254]              32\n",
      "              ReLU-3         [-1, 16, 500, 254]               0\n",
      "            Conv2d-4         [-1, 16, 500, 254]             160\n",
      "            Conv2d-5         [-1, 16, 500, 254]           2,320\n",
      "       BatchNorm2d-6         [-1, 16, 500, 254]              32\n",
      "              ReLU-7         [-1, 16, 500, 254]               0\n",
      "         MaxPool2d-8         [-1, 16, 250, 127]               0\n",
      "            Conv2d-9         [-1, 16, 250, 127]             160\n",
      "           Conv2d-10         [-1, 32, 250, 127]           4,640\n",
      "      BatchNorm2d-11         [-1, 32, 250, 127]              64\n",
      "             ReLU-12         [-1, 32, 250, 127]               0\n",
      "        MaxPool2d-13          [-1, 32, 125, 63]               0\n",
      "           Conv2d-14          [-1, 32, 125, 63]             320\n",
      "           Conv2d-15          [-1, 64, 125, 63]          18,496\n",
      "      BatchNorm2d-16          [-1, 64, 125, 63]             128\n",
      "             ReLU-17          [-1, 64, 125, 63]               0\n",
      "         Upsample-18         [-1, 64, 250, 126]               0\n",
      "           Conv2d-19         [-1, 32, 250, 126]          18,464\n",
      "      BatchNorm2d-20         [-1, 32, 250, 126]              64\n",
      "             ReLU-21         [-1, 32, 250, 126]               0\n",
      "         Upsample-22         [-1, 32, 500, 252]               0\n",
      "           Conv2d-23         [-1, 16, 500, 252]           4,624\n",
      "      BatchNorm2d-24         [-1, 16, 500, 252]              32\n",
      "             ReLU-25         [-1, 16, 500, 252]               0\n",
      "           Conv2d-26          [-1, 1, 500, 252]             145\n",
      "================================================================\n",
      "Total params: 50,481\n",
      "Trainable params: 50,481\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.94\n",
      "Forward/backward pass size (MB): 271.22\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 273.36\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 181, 181]           1,088\n",
      "         LeakyReLU-2         [-1, 64, 181, 181]               0\n",
      "            Conv2d-3          [-1, 128, 90, 90]         131,200\n",
      "       BatchNorm2d-4          [-1, 128, 90, 90]             256\n",
      "         LeakyReLU-5          [-1, 128, 90, 90]               0\n",
      " AdaptiveAvgPool2d-6            [-1, 128, 1, 1]               0\n",
      "            Linear-7                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 132,673\n",
      "Trainable params: 132,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 55.72\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 56.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "generator = UltraLightUNetGenerator().to(device)\n",
    "summary(generator, input_size=(1, 1000, 508))  # Channels, Height, Width\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "summary(discriminator, input_size=(1, 362, 362))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a946f-5b8c-465f-a949-e82d04234764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
