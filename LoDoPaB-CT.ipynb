{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6569906-2052-466e-825a-a852f120ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import random\n",
    "from einops import rearrange  # Ensure this module is installed\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b86d23dd-7ae6-434f-8ee2-e9be52dfb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters & Configuration\n",
    "config = {\n",
    "    'batch_size': 8,\n",
    "    'lr': 1e-4,\n",
    "    'num_epochs': 50,\n",
    "    'num_workers': 2,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'channels': [16, 32, 64],\n",
    "    'kernels': [1, 3],\n",
    "    'expansion_factor': 2,\n",
    "    'reduction_ratio': 16,\n",
    "    'save_dir': './checkpoints',\n",
    "    'data_path': r\"D:\\LoDoPaB-CT\\observation_train\\observation_train_001.hdf5\",  # Ensure this line ends with a comma or nothing\n",
    "    'data_percentage': 0.05  # No comma at the end if it's the last element\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b38f6b-1d6d-4ab5-b519-97af2bb167ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create save directory\n",
    "os.makedirs(config['save_dir'], exist_ok=True)\n",
    "\n",
    "# Ensure dataset file exists\n",
    "if not os.path.exists(config['data_path']):\n",
    "    raise FileNotFoundError(f\"Dataset file not found: {config['data_path']}\")\n",
    "\n",
    "## Model Components (Efficient & Compact)\n",
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self, groups):\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.size()\n",
    "        channels_per_group = num_channels // self.groups\n",
    "        x = x.view(batch_size, self.groups, channels_per_group, height, width)\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        x = x.view(batch_size, -1, height, width)\n",
    "        return x\n",
    "\n",
    "class MultiKernelDepthwiseConv(nn.Module):\n",
    "    def __init__(self, in_channels, kernels=[1, 3]):\n",
    "        super().__init__()\n",
    "        self.kernels = kernels\n",
    "        self.groups = len(kernels)\n",
    "        \n",
    "        self.dw_convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels // self.groups, in_channels // self.groups, kernel_size=k, padding=k//2, \n",
    "                          groups=in_channels // self.groups, bias=False),\n",
    "                nn.BatchNorm2d(in_channels // self.groups),\n",
    "                nn.ReLU6(inplace=True)\n",
    "            ) for k in kernels\n",
    "        ])\n",
    "        \n",
    "        self.channel_shuffle = ChannelShuffle(self.groups)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        splits = torch.split(x, x.size(1) // self.groups, dim=1)\n",
    "        out = [conv(splits[i]) for i, conv in enumerate(self.dw_convs)]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        return self.channel_shuffle(out)\n",
    "\n",
    "class ConvolutionalMultiFocalAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        reduced_channels = max(1, in_channels // reduction_ratio)\n",
    "        \n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, reduced_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(reduced_channels, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ca = self.channel_attention(x)\n",
    "        x_ca = x * ca\n",
    "        max_pool = torch.max(x_ca, dim=1, keepdim=True)[0]\n",
    "        avg_pool = torch.mean(x_ca, dim=1, keepdim=True)\n",
    "        spatial_pool = torch.cat([max_pool, avg_pool], dim=1)\n",
    "        sa = self.spatial_attention(spatial_pool)\n",
    "        return x_ca * sa\n",
    "\n",
    "## Simplified UltraLightUNet\n",
    "class UltraLightUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, channels=[16, 32, 64]):\n",
    "        super().__init__()\n",
    "        self.encoder1 = nn.Conv2d(in_channels, channels[0], kernel_size=3, padding=1)\n",
    "        self.encoder2 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(channels[0], channels[1], kernel_size=3, padding=1))\n",
    "        self.encoder3 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(channels[1], channels[2], kernel_size=3, padding=1))\n",
    "\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(channels[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        d2 = self.up2(e3)\n",
    "        d1 = self.up1(d2)\n",
    "        return self.final_conv(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536f219-5a54-4199-b7c6-fa341a2e11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Data Loaders\n",
    "# train_dataset = LoDoPaBDataset(config['data_path'], mode='train')\n",
    "# val_dataset = LoDoPaBDataset(config['data_path'], mode='val')\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "249a96a5-58a5-4ed6-8da8-8cd9b896fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoDoPaBTrainDataset(Dataset):\n",
    "    def __init__(self, observations_dir, transform=None):\n",
    "        self.observation_files = sorted(glob.glob(os.path.join(observations_dir, \"*.hdf5\")))\n",
    "        self.transform = transform\n",
    "        self.sample_indices = []  # (file_idx, sample_idx)\n",
    "        \n",
    "        for file_idx, obs_file in enumerate(self.observation_files):\n",
    "            with h5py.File(obs_file, 'r') as f:\n",
    "                num_samples = f['data'].shape[0]\n",
    "                self.sample_indices.extend([(file_idx, i) for i in range(num_samples)])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, sample_idx = self.sample_indices[idx]\n",
    "        with h5py.File(self.observation_files[file_idx], 'r') as f:\n",
    "            observation = torch.from_numpy(f['data'][sample_idx]).float().unsqueeze(0)\n",
    "            if self.transform:\n",
    "                observation = self.transform(observation)\n",
    "            return observation  # (1, H, W)\n",
    "\n",
    "class LoDoPaBValDataset(Dataset):\n",
    "    def __init__(self, observations_dir, ground_truth_dir, transform=None):\n",
    "        self.observation_files = sorted(glob.glob(os.path.join(observations_dir, \"*.hdf5\")))\n",
    "        self.ground_truth_files = sorted(glob.glob(os.path.join(ground_truth_dir, \"*.hdf5\")))\n",
    "        self.transform = transform\n",
    "        self.sample_indices = []\n",
    "        \n",
    "        # Verify matching files\n",
    "        assert len(self.observation_files) == len(self.ground_truth_files)\n",
    "        \n",
    "        for file_idx, (obs_file, gt_file) in enumerate(zip(self.observation_files, self.ground_truth_files)):\n",
    "            with h5py.File(obs_file, 'r') as f_obs, h5py.File(gt_file, 'r') as f_gt:\n",
    "                assert f_obs['data'].shape == f_gt['data'].shape\n",
    "                num_samples = f_obs['data'].shape[0]\n",
    "                self.sample_indices.extend([(file_idx, i) for i in range(num_samples)])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, sample_idx = self.sample_indices[idx]\n",
    "        with h5py.File(self.observation_files[file_idx], 'r') as f_obs, \\\n",
    "             h5py.File(self.ground_truth_files[file_idx], 'r') as f_gt:\n",
    "            \n",
    "            obs = torch.from_numpy(f_obs['data'][sample_idx]).float().unsqueeze(0)\n",
    "            gt = torch.from_numpy(f_gt['data'][sample_idx]).float().unsqueeze(0)\n",
    "            \n",
    "            if self.transform:\n",
    "                # Apply same transform to both\n",
    "                stacked = torch.cat([obs, gt], dim=0)\n",
    "                stacked = self.transform(stacked)\n",
    "                obs, gt = stacked[0], stacked[1]\n",
    "            \n",
    "            return obs, gt  # (1, H, W), (1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a9b8e86-0420-4709-9697-bafd805c70c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m LoDoPaBTrainDataset(\n\u001b[0;32m      2\u001b[0m     observations_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/LoDoPaB-CT/observation_train/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Validation (supervised)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m LoDoPaBValDataset(\n\u001b[0;32m      7\u001b[0m     observations_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/LoDoPaB-CT/observation_test/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     ground_truth_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/LoDoPaB-CT/ground_truth_validation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn[43], line 35\u001b[0m, in \u001b[0;36mLoDoPaBValDataset.__init__\u001b[1;34m(self, observations_dir, ground_truth_dir, transform)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_idx, (obs_file, gt_file) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_files, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mground_truth_files)):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(obs_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_obs, h5py\u001b[38;5;241m.\u001b[39mFile(gt_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_gt:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m f_obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m f_gt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     36\u001b[0m         num_samples \u001b[38;5;241m=\u001b[39m f_obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_indices\u001b[38;5;241m.\u001b[39mextend([(file_idx, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)])\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = LoDoPaBTrainDataset(\n",
    "    observations_dir=\"/LoDoPaB-CT/observation_train/\"\n",
    ")\n",
    "\n",
    "# Validation (supervised)\n",
    "val_dataset = LoDoPaBValDataset(\n",
    "    observations_dir=\"/LoDoPaB-CT/observation_test/\",\n",
    "    ground_truth_dir=\"/LoDoPaB-CT/ground_truth_validation/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1569ea6f-3cc4-42d9-8dd4-78b5986d2136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 28 HDF5 files in /LoDoPaB-CT/observation_test/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:10<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All files are valid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm  # for progress bar (install with: pip install tqdm)\n",
    "\n",
    "def check_hdf5_corruption(filepath):\n",
    "    try:\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # Attempt to access a key (e.g., 'data') to verify readability\n",
    "            _ = f['data'][:]  # Try reading a small chunk if files are large\n",
    "        return True  # File is OK\n",
    "    except (OSError, KeyError) as e:\n",
    "        print(f\"\\nCorrupted file: {filepath} | Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def scan_directory_for_corrupt_files(directory):\n",
    "    corrupt_files = []\n",
    "    hdf5_files = [f for f in os.listdir(directory) if f.endswith('.hdf5')]\n",
    "    \n",
    "    print(f\"Checking {len(hdf5_files)} HDF5 files in {directory}...\")\n",
    "    for filename in tqdm(hdf5_files):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if not check_hdf5_corruption(filepath):\n",
    "            corrupt_files.append(filename)\n",
    "    \n",
    "    if not corrupt_files:\n",
    "        print(\"\\n All files are valid.\")\n",
    "    else:\n",
    "        print(f\"\\n Found {len(corrupt_files)} corrupt files:\")\n",
    "        for f in corrupt_files:\n",
    "            print(f\"  - {f}\")\n",
    "\n",
    "# Usage\n",
    "scan_directory_for_corrupt_files(\"/LoDoPaB-CT/observation_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b104ab8f-d9df-4f1b-ae93-b0cd299c1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ground_truth_test.zip: 100%|██████████████████████████████████████████████████████| 1.47G/1.47G [17:17<00:00, 1.52MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ./LoDoPaB-CT\\ground_truth_test.zip\n",
      "All downloads complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm  # For progress bar (install with `pip install tqdm`)\n",
    "\n",
    "# Zenodo record URL (LoDoPaB-CT)\n",
    "ZENODO_RECORD = \"https://zenodo.org/record/3384092\"\n",
    "DOWNLOAD_DIR = \"./LoDoPaB-CT\"  # Where to save files\n",
    "\n",
    "# List of files to download (modify based on your needs)\n",
    "FILES_TO_DOWNLOAD = [\n",
    "    \"ground_truth_test.zip\",\n",
    "    # Add more files as needed\n",
    "]\n",
    "\n",
    "def download_from_zenodo(file_name, save_dir):\n",
    "    \"\"\"Download a file from Zenodo.\"\"\"\n",
    "    download_url = f\"{ZENODO_RECORD}/files/{file_name}?download=1\"\n",
    "    save_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Stream download to handle large files\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "    \n",
    "    # Progress bar\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with open(save_path, 'wb') as f, tqdm(\n",
    "        desc=file_name,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "    \n",
    "    print(f\"Downloaded: {save_path}\")\n",
    "\n",
    "# Download all files\n",
    "for file in FILES_TO_DOWNLOAD:\n",
    "    download_from_zenodo(file, DOWNLOAD_DIR)\n",
    "\n",
    "print(\"All downloads complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cb6c195-c31b-49f4-8c45-c0008d0c0486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 128\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataset\n",
    "dataset = LoDoPaBCTDataset(file_path=\"/LoDoPaB-CT/observation_train/observation_train_000.hdf5\")\n",
    "\n",
    "# Create data loader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "print(f\"Training dataset size: {len(dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb8675-ecd8-4ef2-9990-f7ffcdd8ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Setup\n",
    "model = UltraLightUNet().to(config['device'])\n",
    "optimizer = AdamW(model.parameters(), lr=config['lr'])\n",
    "criterion = nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "## Training Loop\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "    model.train()\n",
    "    train_loss = sum(criterion(model(low.to(config['device'])), full.to(config['device'])).item() \n",
    "                     for low, full in train_loader) / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = sum(criterion(model(low.to(config['device'])), full.to(config['device'])).item() \n",
    "                   for low, full in val_loader) / len(val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(config['save_dir'], 'best_model.pth'))\n",
    "        print(\"Saved new best model!\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00594df-b166-4fda-9e7b-fa66c1982252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
