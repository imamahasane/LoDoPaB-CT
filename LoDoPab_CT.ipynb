{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg16\n",
    "from piqa import SSIM, LPIPS\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths to your dataset folders (adjust to your actual directory)\n",
    "BASE_PATH = \"\"  # Replace with your actual path\n",
    "\n",
    "# TRAIN_OBS_DIR = os.path.join(BASE_PATH, \"observation_train\")\n",
    "# TRAIN_GT_DIR = os.path.join(BASE_PATH, \"ground_truth_train\")\n",
    "\n",
    "TRAIN_OBS_DIR = os.path.join(BASE_PATH, \"observation_test\")\n",
    "TRAIN_GT_DIR = os.path.join(BASE_PATH, \"ground_truth_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for LoDoPaB-CT\n",
    "class LoDoPaBDataset(Dataset):\n",
    "    def __init__(self, obs_dir, gt_dir, num_files=None):\n",
    "        self.obs_files = sorted(os.listdir(obs_dir))\n",
    "        if num_files:\n",
    "            self.obs_files = self.obs_files[:num_files]\n",
    "        self.obs_dir = obs_dir\n",
    "        self.gt_dir = gt_dir\n",
    "\n",
    "        print(\"Loading dataset...\")\n",
    "        self.obs_data = []\n",
    "        self.gt_data = []\n",
    "        for file_name in self.obs_files:\n",
    "            obs_file = os.path.join(obs_dir, file_name)\n",
    "            gt_file = os.path.join(gt_dir, file_name.replace(\"observation\", \"ground_truth\"))\n",
    "            with h5py.File(obs_file, 'r') as f_obs, h5py.File(gt_file, 'r') as f_gt:\n",
    "                obs = f_obs['data'][:].astype(np.float16)\n",
    "                gt = f_gt['data'][:].astype(np.float16)\n",
    "                self.obs_data.append(obs)\n",
    "                self.gt_data.append(gt)\n",
    "        \n",
    "        self.obs_data = np.concatenate(self.obs_data, axis=0)\n",
    "        self.gt_data = np.concatenate(self.gt_data, axis=0)\n",
    "        \n",
    "        self.obs_data = (self.obs_data - np.min(self.obs_data)) / (np.max(self.obs_data) - np.min(self.obs_data)) * 2 - 1\n",
    "        self.gt_data = (self.gt_data - np.min(self.gt_data)) / (np.max(self.gt_data) - np.min(self.gt_data)) * 2 - 1\n",
    "        print(f\"Dataset loaded: {self.obs_data.shape[0]} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obs = torch.FloatTensor(self.obs_data[idx]).unsqueeze(0)  # [1, 1000, 513]\n",
    "        gt = torch.FloatTensor(self.gt_data[idx]).unsqueeze(0)    # [1, 362, 362]\n",
    "        return obs, gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache clearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 3553 samples\n",
      "Dataset size: 3553 samples\n",
      "Number of batches: 297\n"
     ]
    }
   ],
   "source": [
    "dataset = LoDoPaBDataset(TRAIN_OBS_DIR, TRAIN_GT_DIR)\n",
    "dataloader = DataLoader(dataset, batch_size=12, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Test dataloader\n",
    "# print(\"Testing dataloader...\")\n",
    "# for i, (obs, gt) in enumerate(dataloader):\n",
    "#     print(f\"Batch {i+1}/{len(dataloader)} - Obs shape: {obs.shape}, GT shape: {gt.shape}\")\n",
    "#     break\n",
    "# print(\"Dataloader test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiKernelDepthwiseConv(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalMultiFocalAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.conv(x)) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, img_size=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # First two convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Global average pooling (to reduce the feature map size)\n",
    "        self.global_pooling = nn.AdaptiveAvgPool2d(1)  # Outputs a 1x1 feature map per channel\n",
    "        \n",
    "        # Fully connected layer after the global pooling\n",
    "        self.fc = nn.Linear(128, 1)  # The output size is the number of channels after pooling\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        \n",
    "        # Apply global average pooling\n",
    "        x = self.global_pooling(x)\n",
    "        \n",
    "        # Flatten the output of the global pooling (this will be 2D now)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, channels)\n",
    "        \n",
    "        x = self.fc(x)  # Apply the fully connected layer\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraLightUNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, channels=[16, 32, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channels[0], kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.encoder1 = self._make_encoder_block(channels[0], channels[0])\n",
    "        self.encoder2 = self._make_encoder_block(channels[0], channels[1], downsample=True)\n",
    "        self.encoder3 = self._make_encoder_block(channels[1], channels[2], downsample=True)\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(channels[2], channels[1], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels[1]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(channels[1], channels[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(channels[0], out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def _make_encoder_block(self, in_ch, out_ch, downsample=False):\n",
    "        layers = []\n",
    "        if downsample:\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        layers.extend([\n",
    "            nn.Conv2d(in_ch, in_ch, kernel_size=3, padding=1, groups=in_ch),  # Depthwise Conv\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_size = (x.size(2), x.size(3))  # Save input size\n",
    "        x = self.initial_conv(x)  # [8, 16, 256, 256]\n",
    "        e1 = self.encoder1(x)     # [8, 16, 256, 256]\n",
    "        e2 = self.encoder2(e1)    # [8, 32, 128, 128]\n",
    "        e3 = self.encoder3(e2)    # [8, 64, 64, 64]\n",
    "        \n",
    "        d2 = self.up2(e3)         # [8, 32, 128, 128]\n",
    "        e2_resized = F.interpolate(e2, size=(d2.size(2), d2.size(3)), mode='bilinear', align_corners=True)\n",
    "        d2 = d2 + e2_resized      # [8, 32, 128, 128]\n",
    "        \n",
    "        d1 = self.up1(d2)         # [8, 16, 256, 256]\n",
    "        e1_resized = F.interpolate(e1, size=(d1.size(2), d1.size(3)), mode='bilinear', align_corners=True)\n",
    "        d1 = d1 + e1_resized      # [8, 16, 256, 256]\n",
    "        \n",
    "        output = self.final_conv(d1)  # [8, 1, 256, 256]\n",
    "        output = F.interpolate(output, size=original_size, mode='bilinear', align_corners=True)  # [8, 1, 513, 513]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Losses\n",
    "class GANLosses:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.adv_loss = nn.BCELoss()\n",
    "        self.vgg = vgg16(pretrained=True).features[:16].to(device).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.pixel_loss = nn.L1Loss()\n",
    "        self.ssim = SSIM().to(device)\n",
    "        self.lpips = LPIPS().to(device)\n",
    "        \n",
    "    def adversarial_loss(self, pred, is_real):\n",
    "        target = torch.ones_like(pred) if is_real else torch.zeros_like(pred)\n",
    "        return self.adv_loss(pred, target)\n",
    "    \n",
    "    def perceptual_loss(self, generated, target):\n",
    "        gen_rgb = generated.repeat(1, 3, 1, 1)\n",
    "        target_rgb = target.repeat(1, 3, 1, 1)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)\n",
    "        gen_rgb = (gen_rgb - mean) / std\n",
    "        target_rgb = (target_rgb - mean) / std\n",
    "        gen_features = self.vgg(gen_rgb)\n",
    "        target_features = self.vgg(target_rgb)\n",
    "        return F.mse_loss(gen_features, target_features)\n",
    "    \n",
    "    def compute_metrics(self, generated, target):\n",
    "        with torch.no_grad():\n",
    "            gen = (generated + 1) / 2\n",
    "            tgt = (target + 1) / 2\n",
    "            psnr = 10 * torch.log10(1 / F.mse_loss(gen, tgt))\n",
    "            ssim_val = self.ssim(gen, tgt)\n",
    "            lpips_val = self.lpips(gen, tgt)\n",
    "            return {'PSNR': psnr.item(), 'SSIM': ssim_val.item(), 'LPIPS': lpips_val.item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, loss_fn, optimizer_G, optimizer_D, dataloader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()  # Start timer for epoch\n",
    "\n",
    "        for batch in dataloader:\n",
    "            real_images = batch[0].to(device)  # Ensure tensor, not list\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_images = generator(torch.randn_like(real_images))\n",
    "\n",
    "            # --- Discriminator Update ---\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = loss_fn.adversarial_loss(discriminator(real_images), True)\n",
    "            fake_loss = loss_fn.adversarial_loss(discriminator(fake_images.detach()), False)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --- Generator Update ---\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = loss_fn.adversarial_loss(discriminator(fake_images), True) + loss_fn.pixel_loss(fake_images, real_images)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        epoch_time = time.time() - start_time  # Calculate epoch duration\n",
    "\n",
    "        # Print epoch stats with time\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "            f\"D Loss: {d_loss.item():.4f}, \"\n",
    "            f\"G Loss: {g_loss.item():.4f}, \"\n",
    "            f\"Time: {epoch_time:.2f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UltraLightUNetGenerator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "loss_fn = GANLosses(device)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], D Loss: 0.5250, G Loss: 1.3356, Time: 87.81s\n",
      "Epoch [2/5], D Loss: 0.5125, G Loss: 1.2838, Time: 88.41s\n"
     ]
    }
   ],
   "source": [
    "train_gan(generator, discriminator, loss_fn, optimizer_G, optimizer_D, dataloader, 5, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 1, 513, 513])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UltraLightUNetGenerator().to(device)\n",
    "sample_input = torch.randn(8, 1, 513, 513).to(device)\n",
    "output = generator(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be [8, 1, 513, 513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UltraLightUNetGenerator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.randn(8, 1, 512, 512).to(device)\n",
    "output = generator(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 500, 254]             800\n",
      "       BatchNorm2d-2         [-1, 16, 500, 254]              32\n",
      "              ReLU-3         [-1, 16, 500, 254]               0\n",
      "            Conv2d-4         [-1, 16, 500, 254]             160\n",
      "            Conv2d-5         [-1, 16, 500, 254]           2,320\n",
      "       BatchNorm2d-6         [-1, 16, 500, 254]              32\n",
      "              ReLU-7         [-1, 16, 500, 254]               0\n",
      "         MaxPool2d-8         [-1, 16, 250, 127]               0\n",
      "            Conv2d-9         [-1, 16, 250, 127]             160\n",
      "           Conv2d-10         [-1, 32, 250, 127]           4,640\n",
      "      BatchNorm2d-11         [-1, 32, 250, 127]              64\n",
      "             ReLU-12         [-1, 32, 250, 127]               0\n",
      "        MaxPool2d-13          [-1, 32, 125, 63]               0\n",
      "           Conv2d-14          [-1, 32, 125, 63]             320\n",
      "           Conv2d-15          [-1, 64, 125, 63]          18,496\n",
      "      BatchNorm2d-16          [-1, 64, 125, 63]             128\n",
      "             ReLU-17          [-1, 64, 125, 63]               0\n",
      "         Upsample-18         [-1, 64, 250, 126]               0\n",
      "           Conv2d-19         [-1, 32, 250, 126]          18,464\n",
      "      BatchNorm2d-20         [-1, 32, 250, 126]              64\n",
      "             ReLU-21         [-1, 32, 250, 126]               0\n",
      "         Upsample-22         [-1, 32, 500, 252]               0\n",
      "           Conv2d-23         [-1, 16, 500, 252]           4,624\n",
      "      BatchNorm2d-24         [-1, 16, 500, 252]              32\n",
      "             ReLU-25         [-1, 16, 500, 252]               0\n",
      "           Conv2d-26          [-1, 1, 500, 252]             145\n",
      "================================================================\n",
      "Total params: 50,481\n",
      "Trainable params: 50,481\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.94\n",
      "Forward/backward pass size (MB): 271.22\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 273.36\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 181, 181]           1,088\n",
      "         LeakyReLU-2         [-1, 64, 181, 181]               0\n",
      "            Conv2d-3          [-1, 128, 90, 90]         131,200\n",
      "       BatchNorm2d-4          [-1, 128, 90, 90]             256\n",
      "         LeakyReLU-5          [-1, 128, 90, 90]               0\n",
      " AdaptiveAvgPool2d-6            [-1, 128, 1, 1]               0\n",
      "            Linear-7                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 132,673\n",
      "Trainable params: 132,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 55.72\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 56.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generator = UltraLightUNetGenerator().to(device)\n",
    "summary(generator, input_size=(1, 1000, 508))  # Channels, Height, Width\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "summary(discriminator, input_size=(1, 362, 362))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
